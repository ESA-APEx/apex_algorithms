name: "Benchmarks"

on:
  workflow_dispatch:
  schedule:
      - cron: "37 12 * * *"

jobs:
  benchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install benchmark dependencies
        run: |
          python -m pip install qa/tools
          python -m pip install -r qa/benchmarks/requirements.txt
      - name: Test with pytest
        run: |
          cd qa/benchmarks
          mkdir report
          mkdir tmp_path_root
          pytest \
            --random-subset=1 \
            -k max_ndvi_fail_intentionally \
            --html report/report.html --self-contained-html \
            --track-metrics-report=report/metrics.json \
            --basetemp=tmp_path_root \
            --upload-assets-run-id="gh-$GITHUB_RUN_ID" \
            --upload-assets-endpoint-url="https://s3.waw3-1.cloudferro.com" \
            --upload-assets-bucket="APEx-benchmarks"
        env:
          OPENEO_AUTH_METHOD: client_credentials
          OPENEO_AUTH_CLIENT_CREDENTIALS_CDSEFED: ${{ secrets.OPENEO_AUTH_CLIENT_CREDENTIALS_CDSEFED }}
          UPLOAD_ASSETS_ACCESS_KEY_ID: ${{ secrets.UPLOAD_ASSETS_ACCESS_KEY_ID }}
          UPLOAD_ASSETS_SECRET_ACCESS_KEY: ${{ secrets.UPLOAD_ASSETS_SECRET_ACCESS_KEY }}
      - name: List local reports
        if: always()
        run: ls -alR qa/benchmarks/report
      - name: List local results
        if: always()
        run: ls -alR qa/benchmarks/tmp_path_root
      - name: upload report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: report
          path: qa/benchmarks/report/
