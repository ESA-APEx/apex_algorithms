{
  "process_graph": {
    "loadcollection1": {
      "process_id": "load_collection",
      "arguments": {
        "bands": [
          "B02",
          "B03",
          "B04",
          "B05",
          "B06",
          "B07",
          "B08",
          "B11",
          "B12"
        ],
        "id": "SENTINEL2_L2A",
        "properties": {
          "eo:cloud_cover": {
            "process_graph": {
              "lte1": {
                "process_id": "lte",
                "arguments": {
                  "x": {
                    "from_parameter": "value"
                  },
                  "y": 85
                },
                "result": true
              }
            }
          }
        },
        "spatial_extent": {
          "west": 716200,
          "south": 605530,
          "east": 732380,
          "north": 622410,
          "crs": "EPSG:32630"
        },
        "temporal_extent": [
          "2023-01-01",
          "2024-01-01"
        ]
      }
    },
    "resamplespatial1": {
      "process_id": "resample_spatial",
      "arguments": {
        "align": "upper-left",
        "data": {
          "from_node": "loadcollection1"
        },
        "method": "near",
        "projection": "EPSG:32630",
        "resolution": 10
      }
    },
    "loadcollection2": {
      "process_id": "load_collection",
      "arguments": {
        "bands": [
          "SCL"
        ],
        "id": "SENTINEL2_L2A",
        "properties": {
          "eo:cloud_cover": {
            "process_graph": {
              "lte2": {
                "process_id": "lte",
                "arguments": {
                  "x": {
                    "from_parameter": "value"
                  },
                  "y": 85
                },
                "result": true
              }
            }
          }
        },
        "spatial_extent": {
          "west": 716200,
          "south": 605530,
          "east": 732380,
          "north": 622410,
          "crs": "EPSG:32630"
        },
        "temporal_extent": [
          "2023-01-01",
          "2024-01-01"
        ]
      }
    },
    "resamplespatial2": {
      "process_id": "resample_spatial",
      "arguments": {
        "align": "upper-left",
        "data": {
          "from_node": "loadcollection2"
        },
        "method": "near",
        "projection": "EPSG:32630",
        "resolution": 10
      }
    },
    "toscldilationmask1": {
      "process_id": "to_scl_dilation_mask",
      "arguments": {
        "data": {
          "from_node": "resamplespatial2"
        }
      }
    },
    "mask1": {
      "process_id": "mask",
      "arguments": {
        "data": {
          "from_node": "resamplespatial1"
        },
        "mask": {
          "from_node": "toscldilationmask1"
        }
      }
    },
    "aggregatetemporalperiod1": {
      "process_id": "aggregate_temporal_period",
      "arguments": {
        "data": {
          "from_node": "mask1"
        },
        "period": "year",
        "reducer": {
          "process_graph": {
            "median1": {
              "process_id": "median",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                }
              },
              "result": true
            }
          }
        }
      }
    },
    "loadcollection3": {
      "process_id": "load_collection",
      "arguments": {
        "bands": [
          "VV",
          "VH"
        ],
        "id": "SENTINEL1_GLOBAL_MOSAICS",
        "spatial_extent": {
          "west": 716200,
          "south": 605530,
          "east": 732380,
          "north": 622410,
          "crs": "EPSG:32630"
        },
        "temporal_extent": [
          "2022-12-31",
          "2024-01-01"
        ]
      }
    },
    "resamplespatial3": {
      "process_id": "resample_spatial",
      "arguments": {
        "align": "upper-left",
        "data": {
          "from_node": "loadcollection3"
        },
        "method": "near",
        "projection": "EPSG:32630",
        "resolution": 10
      }
    },
    "apply1": {
      "process_id": "apply",
      "arguments": {
        "data": {
          "from_node": "resamplespatial3"
        },
        "process": {
          "process_graph": {
            "log1": {
              "process_id": "log",
              "arguments": {
                "base": 10,
                "x": {
                  "from_parameter": "x"
                }
              }
            },
            "multiply1": {
              "process_id": "multiply",
              "arguments": {
                "x": 10,
                "y": {
                  "from_node": "log1"
                }
              },
              "result": true
            }
          }
        }
      }
    },
    "aggregatetemporalperiod2": {
      "process_id": "aggregate_temporal_period",
      "arguments": {
        "data": {
          "from_node": "apply1"
        },
        "period": "year",
        "reducer": {
          "process_graph": {
            "median2": {
              "process_id": "median",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                }
              },
              "result": true
            }
          }
        }
      }
    },
    "mergecubes1": {
      "process_id": "merge_cubes",
      "arguments": {
        "cube1": {
          "from_node": "aggregatetemporalperiod1"
        },
        "cube2": {
          "from_node": "aggregatetemporalperiod2"
        }
      }
    },
    "loadcollection4": {
      "process_id": "load_collection",
      "arguments": {
        "id": "COPERNICUS_30",
        "spatial_extent": {
          "west": 716200,
          "south": 605530,
          "east": 732380,
          "north": 622410,
          "crs": "EPSG:32630"
        },
        "temporal_extent": null
      }
    },
    "resamplespatial4": {
      "process_id": "resample_spatial",
      "arguments": {
        "align": "upper-left",
        "data": {
          "from_node": "loadcollection4"
        },
        "method": "bilinear",
        "projection": "EPSG:32630",
        "resolution": 10
      }
    },
    "reducedimension1": {
      "process_id": "reduce_dimension",
      "arguments": {
        "data": {
          "from_node": "resamplespatial4"
        },
        "dimension": "t",
        "reducer": {
          "process_graph": {
            "max1": {
              "process_id": "max",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                }
              },
              "result": true
            }
          }
        }
      }
    },
    "mergecubes2": {
      "process_id": "merge_cubes",
      "arguments": {
        "cube1": {
          "from_node": "mergecubes1"
        },
        "cube2": {
          "from_node": "reducedimension1"
        }
      }
    },
    "apply2": {
      "process_id": "apply",
      "arguments": {
        "data": {
          "from_node": "aggregatetemporalperiod1"
        },
        "process": {
          "process_graph": {
            "runudf1": {
              "process_id": "run_udf",
              "arguments": {
                "context": {
                  "west": 716200,
                  "south": 605530,
                  "east": 732380,
                  "north": 622410,
                  "crs": "EPSG:32630"
                },
                "data": {
                  "from_parameter": "x"
                },
                "runtime": "Python",
                "udf": "import numpy as np\nimport xarray as xr\nimport logging\nfrom pyproj import Transformer\nfrom typing import Dict\n\n# Setup logging\ndef _setup_logging() -> logging.Logger:\n    logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n    return logging.getLogger(__name__)\n\nlogger = _setup_logging()\n\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\n    \"\"\"\n    Constructs a lon/lat grid as a new DataArray based on the cube's spatial resolution\n    and the geographic extent provided in `context`.\n\n    Args:\n        cube (xr.DataArray): Input data cube with 'x' and 'y' dimensions.\n        context (dict): Dictionary containing ''crs'.\n\n    Returns:\n        xr.DataArray: A new DataArray of shape (2, y, x) with bands ['lon', 'lat'].\n    \"\"\"\n\n\n    crs   = context[\"crs\"]\n    transformer = Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n    longitudes, latitudes = transformer.transform(cube.x, cube.y)\n    lon_grid, lat_grid = np.meshgrid(longitudes, latitudes)\n\n    logger.info(f\"Cube x range: {cube.x.min().values}, {cube.x.max().values}\")\n    logger.info(f\"Cube y range: {cube.y.min().values}, {cube.y.max().values}\")\n\n    logger.info(f\"Transformed longitudes range: {longitudes.min()}, {longitudes.max()}\")\n    logger.info(f\"Transformed latitudes range: {latitudes.min()}, {latitudes.max()}\")\n\n\n    # Build output DataArray\n    return xr.DataArray(\n        data=np.stack([lon_grid, lat_grid], axis=0),  # shape: (2, y, x)\n        dims=(\"bands\", \"y\", \"x\"),\n        coords={\n            \"bands\": [\"lon\", \"lat\"],\n            \"x\": cube.coords[\"x\"],\n            \"y\": cube.coords[\"y\"]\n        }\n    )"
              },
              "result": true
            }
          }
        }
      }
    },
    "resamplespatial5": {
      "process_id": "resample_spatial",
      "arguments": {
        "align": "upper-left",
        "data": {
          "from_node": "apply2"
        },
        "method": "near",
        "projection": "EPSG:32630",
        "resolution": 10
      }
    },
    "renamelabels1": {
      "process_id": "rename_labels",
      "arguments": {
        "data": {
          "from_node": "resamplespatial5"
        },
        "dimension": "bands",
        "target": [
          "lon",
          "lat"
        ]
      }
    },
    "mergecubes3": {
      "process_id": "merge_cubes",
      "arguments": {
        "cube1": {
          "from_node": "mergecubes2"
        },
        "cube2": {
          "from_node": "renamelabels1"
        }
      }
    },
    "ndvi1": {
      "process_id": "ndvi",
      "arguments": {
        "data": {
          "from_node": "aggregatetemporalperiod1"
        },
        "nir": "B08",
        "red": "B04"
      }
    },
    "adddimension1": {
      "process_id": "add_dimension",
      "arguments": {
        "data": {
          "from_node": "ndvi1"
        },
        "label": "NDVI",
        "name": "bands",
        "type": "bands"
      }
    },
    "reducedimension2": {
      "process_id": "reduce_dimension",
      "arguments": {
        "data": {
          "from_node": "aggregatetemporalperiod1"
        },
        "dimension": "bands",
        "reducer": {
          "process_graph": {
            "arrayelement1": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 6
              }
            },
            "arrayelement2": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 3
              }
            },
            "subtract1": {
              "process_id": "subtract",
              "arguments": {
                "x": {
                  "from_node": "arrayelement1"
                },
                "y": {
                  "from_node": "arrayelement2"
                }
              }
            },
            "arrayelement3": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 6
              }
            },
            "arrayelement4": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 3
              }
            },
            "add1": {
              "process_id": "add",
              "arguments": {
                "x": {
                  "from_node": "arrayelement3"
                },
                "y": {
                  "from_node": "arrayelement4"
                }
              }
            },
            "divide1": {
              "process_id": "divide",
              "arguments": {
                "x": {
                  "from_node": "subtract1"
                },
                "y": {
                  "from_node": "add1"
                }
              },
              "result": true
            }
          }
        }
      }
    },
    "adddimension2": {
      "process_id": "add_dimension",
      "arguments": {
        "data": {
          "from_node": "reducedimension2"
        },
        "label": "NDRE",
        "name": "bands",
        "type": "bands"
      }
    },
    "mergecubes4": {
      "process_id": "merge_cubes",
      "arguments": {
        "cube1": {
          "from_node": "adddimension1"
        },
        "cube2": {
          "from_node": "adddimension2"
        }
      }
    },
    "reducedimension3": {
      "process_id": "reduce_dimension",
      "arguments": {
        "data": {
          "from_node": "aggregatetemporalperiod1"
        },
        "dimension": "bands",
        "reducer": {
          "process_graph": {
            "arrayelement5": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 6
              }
            },
            "arrayelement6": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 2
              }
            },
            "subtract2": {
              "process_id": "subtract",
              "arguments": {
                "x": {
                  "from_node": "arrayelement5"
                },
                "y": {
                  "from_node": "arrayelement6"
                }
              }
            },
            "multiply2": {
              "process_id": "multiply",
              "arguments": {
                "x": 2.5,
                "y": {
                  "from_node": "subtract2"
                }
              }
            },
            "arrayelement7": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 6
              }
            },
            "arrayelement8": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 2
              }
            },
            "multiply3": {
              "process_id": "multiply",
              "arguments": {
                "x": 6,
                "y": {
                  "from_node": "arrayelement8"
                }
              }
            },
            "add2": {
              "process_id": "add",
              "arguments": {
                "x": {
                  "from_node": "arrayelement7"
                },
                "y": {
                  "from_node": "multiply3"
                }
              }
            },
            "arrayelement9": {
              "process_id": "array_element",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "index": 0
              }
            },
            "multiply4": {
              "process_id": "multiply",
              "arguments": {
                "x": 7.5,
                "y": {
                  "from_node": "arrayelement9"
                }
              }
            },
            "subtract3": {
              "process_id": "subtract",
              "arguments": {
                "x": {
                  "from_node": "add2"
                },
                "y": {
                  "from_node": "multiply4"
                }
              }
            },
            "add3": {
              "process_id": "add",
              "arguments": {
                "x": {
                  "from_node": "subtract3"
                },
                "y": 1
              }
            },
            "divide2": {
              "process_id": "divide",
              "arguments": {
                "x": {
                  "from_node": "multiply2"
                },
                "y": {
                  "from_node": "add3"
                }
              },
              "result": true
            }
          }
        }
      }
    },
    "adddimension3": {
      "process_id": "add_dimension",
      "arguments": {
        "data": {
          "from_node": "reducedimension3"
        },
        "label": "EVI",
        "name": "bands",
        "type": "bands"
      }
    },
    "mergecubes5": {
      "process_id": "merge_cubes",
      "arguments": {
        "cube1": {
          "from_node": "mergecubes4"
        },
        "cube2": {
          "from_node": "adddimension3"
        }
      }
    },
    "mergecubes6": {
      "process_id": "merge_cubes",
      "arguments": {
        "cube1": {
          "from_node": "mergecubes3"
        },
        "cube2": {
          "from_node": "mergecubes5"
        }
      }
    },
    "apply3": {
      "process_id": "apply",
      "arguments": {
        "data": {
          "from_node": "mergecubes6"
        },
        "process": {
          "process_graph": {
            "runudf2": {
              "process_id": "run_udf",
              "arguments": {
                "data": {
                  "from_parameter": "x"
                },
                "runtime": "Python",
                "udf": "import numpy as np\nimport xarray as xr\nimport logging\nfrom typing import Dict, Tuple, List\n\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(message)s\")\nlogger = logging.getLogger(__name__)\n\n# Normalization specifications\nNORMALIZATION_SPECS = {\n    \"optical\": {\n        \"B02\": (1.7417268007636313, 2.023298706048351),\n        \"B03\": (1.7261204997060209, 2.038905204308012),\n        \"B04\": (1.6798346251414997, 2.179592821212937),\n        \"B05\": (2.3828939530384052, 2.7578332604178284),\n        \"B06\": (1.7417268007636313, 2.023298706048351),\n        \"B07\": (1.7417268007636313, 2.023298706048351),\n        \"B08\": (1.7417268007636313, 2.023298706048351),\n        \"B11\": (1.7417268007636313, 2.023298706048351),\n        \"B12\": (1.7417268007636313, 2.023298706048351),\n    },\n    \"linear\": {\n        \"VV\": (-25, 0),\n        \"VH\": (-30, -5),\n        \"DEM\": (-400, 8000),\n        \"lon\": (-180, 180),\n        \"lat\": (-60, 60),\n        \"NDVI\": (-1, 1),\n        \"NDRE\": (-1, 1),\n        \"EVI\": (-1, 1)\n    },\n}\n\ndef _normalize_optical(arr: np.ndarray, min_val: float, max_val: float) -> np.ndarray:\n    \"\"\"Log-based normalization for optical bands.\"\"\"\n    arr = np.log(arr * 0.005 + 1)\n    arr = (arr - min_val) / (max_val)\n    arr = np.exp(arr * 5 - 1)\n    return arr / (arr + 1)\n\n\ndef _normalize_linear(arr: np.ndarray, min_val: float, max_val: float) -> np.ndarray:\n    \"\"\"Linear min\u2013max normalization for continuous variables.\"\"\"\n    arr = np.clip(arr, min_val, max_val)\n    return (arr - min_val) / (max_val - min_val)\n\nNORMALIZE_FUNCS = {\n    \"optical\": _normalize_optical,\n    \"linear\": _normalize_linear,\n}\n\ndef get_expected_bands() -> List[str]:\n    \"\"\"\n    Derive expected band order directly from NORMALIZATION_SPECS.\n    Preserves the order in which groups and bands were defined.\n    \"\"\"\n    expected = []\n    for group_bands in NORMALIZATION_SPECS.values():\n        expected.extend(group_bands.keys())\n    return expected\n\ndef validate_bands(cube: xr.DataArray, expected_bands: list):\n    \"\"\"\n    Validate presence and order of required bands in a data cube.\n\n    Ensures that:\n      1. All required bands are present.\n      2. Bands are in the correct order.\n\n    Args:\n        cube (xr.DataArray):\n            Input data cube with a 'bands' coordinate.\n        expected_bands (list):\n            Ordered list of band names required for processing.\n\n    Returns:\n        xr.DataArray:\n            Data cube with bands in the correct order.\n\n    Raises:\n        ValueError: If any required bands are missing.\n    \"\"\"\n    band_names = list(cube.coords[\"bands\"].values)\n    logger.info(f\"Input bands: {band_names}\")\n\n    # Check for missing bands\n    missing_bands = [b for b in expected_bands if b not in band_names]\n    if missing_bands:\n        raise ValueError(f\"Missing required bands: {missing_bands}. Got: {band_names}\")\n\n    # Reorder if needed\n    if band_names != expected_bands:\n        raise ValueError(f\"Band order mismatch: {band_names} vs {expected_bands}\")\n\n\n\ndef apply_datacube(cube: xr.DataArray, context: dict) -> xr.DataArray:\n\n    \"\"\"\n    Normalize all bands in an input data cube according to predefined specifications.\n\n    Steps:\n      1. Derive expected band order from NORMALIZATION_SPECS.\n      2. Validate band presence and order.\n      3. Apply normalization function per band based on its group.\n\n    Args:\n        cube (xr.DataArray):\n            Input data cube with dimensions (\"bands\", \"y\", \"x\").\n\n    Returns:\n        xr.DataArray:\n            Normalized data cube with same shape, dimensions, and band names.\n\n    Raises:\n        ValueError: If required bands are missing or in the wrong order.\n    \"\"\"\n\n    logger.info(f\"Received data with shape: {cube.shape}, dims: {cube.dims}\")\n\n    # --- Validate & reorder bands in one call ---\n    expected_bands = get_expected_bands()\n    validate_bands(cube, expected_bands)\n\n    # --- Normalization logic stays unchanged ---\n    band_names = list(cube.coords[\"bands\"].values)\n    logger.info(f\"Normalizing bands: {band_names}\")\n\n    img_values = cube.values\n    normalized_bands = []\n    output_band_names = []\n    for band in band_names:\n        arr = img_values[band_names.index(band)]\n        pre_stats = (arr.min(), arr.max(), arr.mean())\n        # Find which group this band belongs to\n        group = None\n        for g, specs in NORMALIZATION_SPECS.items():\n            if band in specs:\n                group = g\n                min_val, max_val = specs[band]\n                norm_func = NORMALIZE_FUNCS[group]\n                normalized = norm_func(arr, min_val, max_val)\n                post_stats = (normalized.min(), normalized.max(), normalized.mean())\n                logger.info(\n                    f\"Band {band}: group={group}, \"\n                    f\"min={pre_stats[0]:.3f}->{post_stats[0]:.3f}, \"\n                    f\"max={pre_stats[1]:.3f}->{post_stats[1]:.3f}, \"\n                    f\"mean={pre_stats[2]:.3f}->{post_stats[2]:.3f}\"\n                )\n                normalized_bands.append(normalized)\n                output_band_names.append(band)\n                break\n\n        if group is None:\n            logger.warning(f\"Band {band}: no normalization defined, leaving unchanged.\")\n            post_stats = pre_stats\n            logger.info(\n                f\"Band {band}: kept as-is, \"\n                f\"min={pre_stats[0]:.3f}, max={pre_stats[1]:.3f}, mean={pre_stats[2]:.3f}\"\n            )\n            normalized_bands.append(arr.astype(np.float32))\n            output_band_names.append(band)\n\n    # Stack back into DataArray\n    result_array = np.stack(normalized_bands, axis=0)\n    da = xr.DataArray(\n        result_array,\n        dims=(\"bands\", \"y\", \"x\"),\n        coords={\n            \"bands\": output_band_names,\n            \"x\": cube.coords[\"x\"],\n            \"y\": cube.coords[\"y\"],\n        },\n    )\n    logger.info(f\"Normalization complete. Output bands: {output_band_names}\")\n    return da\n"
              },
              "result": true
            }
          }
        }
      }
    },
    "applyneighborhood1": {
      "process_id": "apply_neighborhood",
      "arguments": {
        "data": {
          "from_node": "apply3"
        },
        "overlap": [
          {
            "dimension": "x",
            "value": 32,
            "unit": "px"
          },
          {
            "dimension": "y",
            "value": 32,
            "unit": "px"
          }
        ],
        "process": {
          "process_graph": {
            "runudf3": {
              "process_id": "run_udf",
              "arguments": {
                "context": {
                  "model_path": "dynamic_models//best_weights_att_unet_lagtime_5_Fused3_2023_totalLoss6V1_without_loss_sentAfrica6.onnx"
                },
                "data": {
                  "from_parameter": "data"
                },
                "runtime": "Python",
                "udf": "import sys\nimport functools\nimport numpy as np\nimport xarray as xr\nimport logging\nfrom typing import Dict, Tuple\nfrom scipy.special import expit\n\n\n# Setup logger\ndef _setup_logging():\n    logging.basicConfig(level=logging.INFO)\n    return logging.getLogger(__name__)\n\nlogger = _setup_logging()\n\n# Add ONNX paths\nsys.path.append(\"onnx_deps\")\nsys.path.append(\"onnx_models\")\nimport onnxruntime as ort\n\n# Constants for sanitization\n_INF_REPLACEMENT = 1e6\n_NEG_INF_REPLACEMENT = -1e6\n\n@functools.lru_cache(maxsize=1)\ndef _load_ort_session(model_name: str) -> ort.InferenceSession:\n    \"\"\"Loads an ONNX model and returns a cached ONNX runtime session.\"\"\"\n    return ort.InferenceSession(f\"onnx_models/{model_name}\")\n\ndef preprocess_image(cube: xr.DataArray) -> Tuple[np.ndarray, Dict[str, xr.Coordinate], np.ndarray]:\n    \"\"\"\n    Prepare the input cube for inference:\n      - Transpose to (y, x, bands)\n      - Sanitize NaN/Inf\n      - Return batch tensor, coords, and invalid-value mask\n    \"\"\"\n    # Reorder dims\n    reordered = cube.transpose(\"y\", \"x\", \"bands\")\n    values = reordered.values.astype(np.float32)\n\n    # Mask invalid entries\n    mask_invalid = ~np.isfinite(values)\n\n    # Replace NaN with 0, inf with large sentinel\n    sanitized = np.where(np.isnan(values), 0.0, values)\n    sanitized = np.where(np.isposinf(sanitized), _INF_REPLACEMENT, sanitized)\n    sanitized = np.where(np.isneginf(sanitized), _NEG_INF_REPLACEMENT, sanitized)\n\n    # Add batch dimension\n    input_tensor = sanitized[None, ...]\n    logger.info(f\"Preprocessed tensor shape={input_tensor.shape}\")\n    return input_tensor, reordered.coords, mask_invalid\n\n\ndef run_inference(\n    session: ort.InferenceSession,\n    input_name: str,\n    input_tensor: np.ndarray\n) -> np.ndarray:\n    \"\"\"Run ONNX session and remove batch dimension from output.\"\"\"\n    outputs = session.run(None, {input_name: input_tensor})\n    pred = np.squeeze(outputs[0], axis=0)\n    logger.info(f\"Inference output shape={pred.shape}\")\n    return pred\n\n#TODO\ndef postprocess_output(\n    pred: np.ndarray,  # Shape: [y, x, bands]\n    coords: Dict[str, xr.Coordinate],\n    mask_invalid: np.ndarray  # Shape: [y, x, bands]\n) -> xr.DataArray:\n    \"\"\"\n    Appends winning class index as new band to predictions:\n      - Keeps original prediction values\n      - Adds new band (-1 for invalid, 0..n-1 for winning class)\n    \"\"\"\n\n    # Apply sigmoid\n    sigmoid_probs = expit(pred)  # shape [y, x, bands]\n\n    # Optionally pick highest prob if needed\n    class_index = np.argmax(sigmoid_probs, axis=-1, keepdims=True)\n\n    # Identify invalid pixels (any invalid in input bands)\n    invalid_mask = np.any(mask_invalid, axis=-1, keepdims=True)\n    class_index = np.where(invalid_mask, -1, class_index).astype(np.float32)\n\n    # Update band coordinates\n    new_band_coords = np.arange(pred.shape[-1] + 1)\n\n    combined = np.concatenate([sigmoid_probs, class_index], axis=-1)\n\n    return xr.DataArray(\n        combined,\n        dims=(\"y\", \"x\", \"bands\"),\n        coords={\n            \"y\": coords[\"y\"],\n            \"x\": coords[\"x\"],\n            \"bands\": new_band_coords\n        },\n        attrs={\"description\": \"Original preds, sigmoid probs, class index\"}\n    )\n\n\n\ndef apply_model(\n    cube: xr.DataArray,\n    model_path: str\n) -> xr.DataArray:\n    \"\"\"\n    Full inference pipeline: preprocess, infer, postprocess.\n    \"\"\"\n    input_tensor, coords, mask_invalid = preprocess_image(cube)\n    session = _load_ort_session(model_path)\n    input_name = session.get_inputs()[0].name\n    raw_pred = run_inference(session, input_name, input_tensor)\n\n    #TODO evaluate reprocessing\n    result = postprocess_output(raw_pred, coords, mask_invalid)\n    #logger.info(f\"apply_model result shape={result.shape}\")\n    return result\n\n\ndef apply_datacube(cube: xr.DataArray, context: dict) -> xr.DataArray:\n    \"\"\"\n    Apply ONNX model per timestep in the datacube.\n    \"\"\"\n    logger.info(f\"apply_datacube received shape={cube.shape}, dims={cube.dims}\")\n\n    model_path  = str(context.get(\"model_path\" ))\n\n    logger.info(f\"Applying model: {model_path}\")\n\n    cube = cube.transpose('y', 'x', 'bands', 't')\n\n    if 't' in cube.dims:\n        logger.info(\"Applying model per timestep via groupby-map.\")\n        return cube.groupby('t').map(lambda da: apply_model(da,  model_path))\n    else:\n        logger.info(\"Single timestep: applying model once.\")\n        return apply_model(cube,  model_path)"
              },
              "result": true
            }
          }
        },
        "size": [
          {
            "dimension": "x",
            "value": 128,
            "unit": "px"
          },
          {
            "dimension": "y",
            "value": 128,
            "unit": "px"
          }
        ]
      }
    },
    "renamelabels2": {
      "process_id": "rename_labels",
      "arguments": {
        "data": {
          "from_node": "applyneighborhood1"
        },
        "dimension": "bands",
        "target": [
          "prob_class_0",
          "prob_class_1",
          "prob_class_2",
          "prob_class_3",
          "prob_class_4",
          "prob_class_5",
          "prob_class_6",
          "prob_class_7",
          "prob_class_8",
          "prob_class_9",
          "prob_class_10",
          "prob_class_11",
          "prob_class_12",
          "prob_class_13",
          "prob_class_14",
          "prob_class_15",
          "prob_class_16",
          "prob_class_17",
          "prob_class_18",
          "prob_class_19",
          "prob_class_20",
          "prob_class_21",
          "prob_class_22",
          "prob_class_23",
          "classification"
        ]
      },
      "result": true
    }
  },
  "id": "WAC_inference",
  "summary": "WAC_inference",
  "default_job_options": {
    "driver-memory": "2000m",
    "driver-memoryOverhead": "2000m",
    "executor-memory": "3000m",
    "executor-memoryOverhead": "3000m",
    "python-memory": "8000m",
    "max-executors": 20,
    "udf-dependency-archives": [
      "https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/onnx_dependencies_1.16.3.zip#onnx_deps",
      "https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/WorldAgriCommodities/dynamic_models.zip#onnx_models"
    ]
  },
  "parameters": [
    {
      "name": "spatial_extent",
      "description": "Limits the data to process to the specified bounding box or polygons.\n\nFor raster data, the process loads the pixel into the data cube if the point\nat the pixel center intersects with the bounding box or any of the polygons\n(as defined in the Simple Features standard by the OGC).\n\nFor vector data, the process loads the geometry into the data cube if the geometry\nis fully within the bounding box or any of the polygons (as defined in the\nSimple Features standard by the OGC). Empty geometries may only be in the\ndata cube if no spatial extent has been provided.\n\nEmpty geometries are ignored.\n\nSet this parameter to null to set no limit for the spatial extent.",
      "schema": [
        {
          "title": "Bounding Box",
          "type": "object",
          "subtype": "bounding-box",
          "required": [
            "west",
            "south",
            "east",
            "north"
          ],
          "properties": {
            "west": {
              "description": "West (lower left corner, coordinate axis 1).",
              "type": "number"
            },
            "south": {
              "description": "South (lower left corner, coordinate axis 2).",
              "type": "number"
            },
            "east": {
              "description": "East (upper right corner, coordinate axis 1).",
              "type": "number"
            },
            "north": {
              "description": "North (upper right corner, coordinate axis 2).",
              "type": "number"
            },
            "base": {
              "description": "Base (optional, lower left corner, coordinate axis 3).",
              "type": [
                "number",
                "null"
              ],
              "default": null
            },
            "height": {
              "description": "Height (optional, upper right corner, coordinate axis 3).",
              "type": [
                "number",
                "null"
              ],
              "default": null
            },
            "crs": {
              "description": "Coordinate reference system of the extent, specified as as [EPSG code](http://www.epsg-registry.org/) or [WKT2 CRS string](http://docs.opengeospatial.org/is/18-010r7/18-010r7.html). Defaults to `4326` (EPSG code 4326) unless the client explicitly requests a different coordinate reference system.",
              "anyOf": [
                {
                  "title": "EPSG Code",
                  "type": "integer",
                  "subtype": "epsg-code",
                  "minimum": 1000,
                  "examples": [
                    3857
                  ]
                },
                {
                  "title": "WKT2",
                  "type": "string",
                  "subtype": "wkt2-definition"
                }
              ],
              "default": 4326
            }
          }
        },
        {
          "title": "Vector data cube",
          "description": "Limits the data cube to the bounding box of the given geometries in the vector data cube. For raster data, all pixels inside the bounding box that do not intersect with any of the polygons will be set to no data (`null`). Empty geometries are ignored.",
          "type": "object",
          "subtype": "datacube",
          "dimensions": [
            {
              "type": "geometry"
            }
          ]
        },
        {
          "title": "No filter",
          "description": "Don't filter spatially. All data is included in the data cube.",
          "type": "null"
        }
      ],
      "default": {
        "west": 716200,
        "south": 605530,
        "east": 732380,
        "north": 622410,
        "crs": "EPSG:32630"
      },
      "optional": true
    },
    {
      "name": "temporal_extent",
      "description": "Temporal extent specified as two-element array with start and end date/date-time.",
      "schema": {
        "type": "array",
        "subtype": "temporal-interval",
        "uniqueItems": true,
        "minItems": 2,
        "maxItems": 2,
        "items": {
          "anyOf": [
            {
              "type": "string",
              "subtype": "date-time",
              "format": "date-time"
            },
            {
              "type": "string",
              "subtype": "date",
              "format": "date"
            },
            {
              "type": "null"
            }
          ]
        }
      },
      "default": [
        "2023-01-01",
        "2024-01-01"
      ],
      "optional": true
    }
  ]
}