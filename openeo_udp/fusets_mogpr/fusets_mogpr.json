{
    "process_graph": {
        "applyneighborhood1": {
            "process_id": "apply_neighborhood",
            "arguments": {
                "data": {
                    "from_parameter": "data"
                },
                "overlap": [],
                "process": {
                    "process_graph": {
                        "runudf1": {
                            "process_id": "run_udf",
                            "arguments": {
                                "context": {},
                                "data": {
                                    "from_parameter": "data"
                                },
                                "runtime": "Python",
                                "udf": "import os\nimport sys\nimport zipfile\nimport requests\nimport functools\nfrom pathlib import Path\n\nfrom openeo.udf import inspect\n\n\ndef download_file(url, path):\n    \"\"\"\n    Downloads a file from the given URL to the specified path.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    with open(path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef extract_zip(zip_path, extract_to):\n    \"\"\"\n    Extracts a zip file from zip_path to the specified extract_to directory.\n    \"\"\"\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_to)\n\n\ndef add_directory_to_sys_path(directory):\n    \"\"\"\n    Adds a directory to the Python sys.path if it's not already present.\n    \"\"\"\n    if directory not in sys.path:\n        sys.path.append(directory)\n\n@functools.lru_cache(maxsize=5)\ndef setup_dependencies(dependencies_url,DEPENDENCIES_DIR):\n    \"\"\"\n    Main function to set up the dependencies by downloading, extracting,\n    and adding necessary directories to sys.path.\n    \"\"\"\n\n    inspect(message=\"Create directories\")\n    # Ensure base directories exist\n    os.makedirs(DEPENDENCIES_DIR, exist_ok=True)\n\n    # Download and extract dependencies if not already present\n    if not os.listdir(DEPENDENCIES_DIR):\n\n        inspect(message=\"Extract dependencies\")\n        zip_path = os.path.join(DEPENDENCIES_DIR, \"temp.zip\")\n        download_file(dependencies_url, zip_path)\n        extract_zip(zip_path, DEPENDENCIES_DIR)\n        os.remove(zip_path)\n\n        # Add the extracted dependencies directory to sys.path\n        add_directory_to_sys_path(DEPENDENCIES_DIR)\n        inspect(message=\"Added to the sys path\")\n\nsetup_dependencies(\"https://artifactory.vgt.vito.be:443/artifactory/auxdata-public/ai4food/fusets_venv.zip\", 'venv')\nsetup_dependencies(\"https://artifactory.vgt.vito.be:443/artifactory/auxdata-public/ai4food/fusets.zip\", 'venv_static')\nimport os\nimport sys\nfrom configparser import ConfigParser\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom openeo.udf import XarrayDataCube\n\n\ndef load_venv():\n    \"\"\"\n    Add the virtual environment to the system path if the folder `/tmp/venv_static` exists\n    :return:\n    \"\"\"\n    for venv_path in ['tmp/venv_static', 'tmp/venv']:\n        if Path(venv_path).exists():\n            sys.path.insert(0, venv_path)\n\n\ndef set_home(home):\n    os.environ['HOME'] = home\n\n\ndef create_gpy_cfg():\n    home = os.getenv('HOME')\n    set_home('/tmp')\n    user_file = Path.home() / '.config' / 'GPy' / 'user.cfg'\n    if not user_file.exists():\n        user_file.parent.mkdir(parents=True, exist_ok=True)\n    return user_file, home\n\n\ndef write_gpy_cfg():\n    user_file, home = create_gpy_cfg()\n    config = ConfigParser()\n    config['plotting'] = {\n        'library': 'none'\n    }\n    with open(user_file, 'w') as cfg:\n        config.write(cfg)\n        cfg.close()\n    return home\n\n\ndef apply_datacube(cube: XarrayDataCube, context: Dict) -> XarrayDataCube:\n    \"\"\"\n    Apply mogpr integration to a datacube.\n    MOGPR requires a full timeseries for multiple bands, so it needs to be invoked in the context of an apply_neighborhood process.\n    @param cube:\n    @param context:\n    @return:\n    \"\"\"\n    load_venv()\n    home = write_gpy_cfg()\n\n    from fusets.mogpr import mogpr\n    dims = cube.get_array().dims\n    result = mogpr(cube.get_array().to_dataset(dim=\"bands\"))\n    result_dc = XarrayDataCube(result.to_array(dim=\"bands\").transpose(*dims))\n    set_home(home)\n    return result_dc\n\n\ndef load_mogpr_udf() -> str:\n    \"\"\"\n    Loads an openEO udf that applies mogpr.\n    @return:\n    \"\"\"\n    import os\n    return Path(os.path.realpath(__file__)).read_text()\n"
                            },
                            "result": true
                        }
                    }
                },
                "size": [
                    {
                        "dimension": "x",
                        "value": 32,
                        "unit": "px"
                    },
                    {
                        "dimension": "y",
                        "value": 32,
                        "unit": "px"
                    }
                ]
            },
            "result": true
        }
    },
    "id": "fusets_mogpr",
    "summary": "Integrates timeseries in data cube using multi-output gaussian process regression",
    "description": "# Multi-output Gaussian process regression (MOGPR)\n\nThe MOGPR service is designed to enable multi-output regression analysis using Gaussian Process Regression (GPR) on geospatial data. It provides a powerful tool for understanding and predicting spatiotemporal phenomena by filling gaps based on other correlated indicators.\n\n## Parameters\n\nThe MOGPR service requires the following parameters:\n\n- `datacube`: The input datacube that contains the data to be gap-filled.\n\n## Usage\n\nThe MOGPR service can be used as follows:\n\n```python\n\nimport openeo\n\n## Setup of parameters\nspat_ext = {\nÂ  Â  \"type\": \"Polygon\",\nÂ  Â  \"coordinates\": [\n [\n [\nÂ  Â  Â  Â  Â  Â  Â  Â  5.170012098271149,\nÂ  Â  Â  Â  Â  Â  Â  Â  51.25062964728295\n ],\n [\nÂ  Â  Â  Â  Â  Â  Â  Â  5.17085904378298,\nÂ  Â  Â  Â  Â  Â  Â  Â  51.24882567194015\n ],\n [\nÂ  Â  Â  Â  Â  Â  Â  Â  5.17857421368097,\nÂ  Â  Â  Â  Â  Â  Â  Â  51.2468515482926\n ],\n [\nÂ  Â  Â  Â  Â  Â  Â  Â  5.178972704726344,\nÂ  Â  Â  Â  Â  Â  Â  Â  51.24982704376254\n ],\n [\nÂ  Â  Â  Â  Â  Â  Â  Â  5.170012098271149,\nÂ  Â  Â  Â  Â  Â  Â  Â  51.25062964728295\n ]\n ]\n ]\n}\ntemp_ext = [\"2022-05-01\", \"2023-07-31\"]\n\n## Setup connection to openEO\neoconn = openeo.connect(\nÂ  Â  Â  Â  \"openeo.dataspace.copernicus.eu\"\n ).authenticate_oidc(\"CDSE\")\n\n## Create a base NDVI datacube that can be used as input for the service\nbase = eoconn.load_collection('SENTINEL2_L2A',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  spatial_extent=spat_ext,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temporal_extent=temp_ext,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bands=[\"B04\", \"B08\", \"SCL\"])\nmask = scl.process(\"to_scl_dilation_mask\", data=scl)\nbase_cloudmasked = base.mask(mask)\nbase_ndvi = base_cloudmasked.ndvi(red=\"B04\", nir=\"B08\")\n\nprocess_id = \"fusets_mogpr\"\nnamespace_url = \"public_url\" Â  Â # publised URL of the process\n## Create a processing graph from the MOGPR process using an active openEO connection\nmogpr = eoconn.datacube_from_process(\nÂ  Â  Â  Â process_id=process_id,\nÂ  Â  Â  Â namespace= namespace_url,\nÂ  Â  Â  data=base_ndvi, \n )\n\n\n## Calculate the average time series value for the given area of interest\nmogpr = mogpr.aggregate_spatial(spat_ext, reducer='mean')\n\n# Execute the service as a batch process\nmogpr_job = mogpr.execute_batch('./mogpr.json', out_format=\"json\", title=f'FuseTS - MOGPR') \n\n```\n\n## Output\n\nThe User-Defined-Process (UDP) produces a datacube that contains a gap-filled time series for all pixels within the specified temporal and spatial range. This datacube can be seamlessly integrated with other openEO processes.",
    "parameters": [
        {
            "name": "data",
            "description": "Raster cube for which to calculate the peaks and valleys",
            "schema": {
                "type": "object",
                "subtype": "datacube"
            }
        }
    ]
}