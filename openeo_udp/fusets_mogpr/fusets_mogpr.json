{
    "process_graph": {
        "applyneighborhood1": {
            "process_id": "apply_neighborhood",
            "arguments": {
                "data": {
                    "from_parameter": "input_raster_cube"
                },
                "overlap": [],
                "process": {
                    "process_graph": {
                        "runudf1": {
                            "process_id": "run_udf",
                            "arguments": {
                                "context": {},
                                "data": {
                                    "from_parameter": "data"
                                },
                                "runtime": "Python",
                                "udf": "import os\nimport sys\nimport zipfile\nimport requests\nimport functools\nfrom typing import Union\nfrom pathlib import Path\n\nfrom openeo.udf import inspect\n\n\n# Example constants for demonstration\nDEPENDENCIES_DIR1 = 'venv'\nDEPENDENCIES_DIR2 = 'venv_static'\n\nDEPENDENCIES_URL1 = \"https://artifactory.vgt.vito.be:443/artifactory/auxdata-public/ai4food/fusets_venv.zip\"\nDEPENDENCIES_URL2 = \"https://artifactory.vgt.vito.be:443/artifactory/auxdata-public/ai4food/fusets.zip\"\n\n\ndef download_file(url, path):\n    \"\"\"\n    Downloads a file from the given URL to the specified path.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    with open(path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef extract_zip(zip_path, extract_to):\n    \"\"\"\n    Extracts a zip file from zip_path to the specified extract_to directory.\n    \"\"\"\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_to)\n    os.remove(zip_path)  # Clean up the zip file after extraction\n\n\ndef add_directory_to_sys_path(directory):\n    \"\"\"\n    Adds a directory to the Python sys.path if it's not already present.\n    \"\"\"\n    if directory not in sys.path:\n        sys.path.insert(0, directory)\n\n@functools.lru_cache(maxsize=5)\ndef setup_dependencies(dependencies_url,DEPENDENCIES_DIR):\n    \"\"\"\n    Main function to set up the dependencies by downloading, extracting,\n    and adding necessary directories to sys.path.\n    \"\"\"\n\n    inspect(message=\"Create directories\")\n    # Ensure base directories exist\n    os.makedirs(DEPENDENCIES_DIR, exist_ok=True)\n\n    # Download and extract dependencies if not already present\n    if not os.listdir(DEPENDENCIES_DIR):\n\n        inspect(message=\"Extract dependencies\")\n        zip_path = os.path.join(DEPENDENCIES_DIR, \"temp.zip\")\n        download_file(dependencies_url, zip_path)\n        extract_zip(zip_path, DEPENDENCIES_DIR)\n\n        # Add the extracted dependencies directory to sys.path\n        add_directory_to_sys_path(DEPENDENCIES_DIR)\n        inspect(message=\"Added to the sys path\")\n\nsetup_dependencies(DEPENDENCIES_URL1, DEPENDENCIES_DIR1)\nsetup_dependencies(DEPENDENCIES_URL2, DEPENDENCIES_DIR2)\n\n\ndef load_set_path() -> str:\n    \"\"\"\n    loads path setup functions \n    @return:\n    \"\"\"\n    import os\n\n    return Path(os.path.realpath(__file__)).read_text()\nimport os\nimport sys\nfrom configparser import ConfigParser\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom openeo.udf import XarrayDataCube\n\n\ndef load_venv():\n    \"\"\"\n    Add the virtual environment to the system path if the folder `/tmp/venv_static` exists\n    :return:\n    \"\"\"\n    for venv_path in ['tmp/venv_static', 'tmp/venv']:\n        if Path(venv_path).exists():\n            sys.path.insert(0, venv_path)\n\n\ndef set_home(home):\n    os.environ['HOME'] = home\n\n\ndef create_gpy_cfg():\n    home = os.getenv('HOME')\n    set_home('/tmp')\n    user_file = Path.home() / '.config' / 'GPy' / 'user.cfg'\n    if not user_file.exists():\n        user_file.parent.mkdir(parents=True, exist_ok=True)\n    return user_file, home\n\n\ndef write_gpy_cfg():\n    user_file, home = create_gpy_cfg()\n    config = ConfigParser()\n    config['plotting'] = {\n        'library': 'none'\n    }\n    with open(user_file, 'w') as cfg:\n        config.write(cfg)\n        cfg.close()\n    return home\n\n\ndef apply_datacube(cube: XarrayDataCube, context: Dict) -> XarrayDataCube:\n    \"\"\"\n    Apply mogpr integration to a datacube.\n    MOGPR requires a full timeseries for multiple bands, so it needs to be invoked in the context of an apply_neighborhood process.\n    @param cube:\n    @param context:\n    @return:\n    \"\"\"\n    load_venv()\n    home = write_gpy_cfg()\n\n    from fusets.mogpr import mogpr\n    dims = cube.get_array().dims\n    result = mogpr(cube.get_array().to_dataset(dim=\"bands\"))\n    result_dc = XarrayDataCube(result.to_array(dim=\"bands\").transpose(*dims))\n    set_home(home)\n    return result_dc\n\n\ndef load_mogpr_udf() -> str:\n    \"\"\"\n    Loads an openEO udf that applies mogpr.\n    @return:\n    \"\"\"\n    import os\n    return Path(os.path.realpath(__file__)).read_text()\n"
                            },
                            "result": true
                        }
                    }
                },
                "size": [
                    {
                        "dimension": "x",
                        "value": 32,
                        "unit": "px"
                    },
                    {
                        "dimension": "y",
                        "value": 32,
                        "unit": "px"
                    }
                ]
            },
            "result": true
        }
    },
    "id": "fusets_mogpr",
    "summary": "Integrates timeseries in data cube using multi-output gaussian process regression",
    "description": "# Multi output gaussian process regression\n\n## Description\n\nCompute an integrated timeseries based on multiple inputs.\nFor instance, combine Sentinel-2 NDVI with Sentinel-1 RVI into one integrated NDVI.\n\n## Limitations\n\nThe spatial extent is limited to a maximum size equal to a Sentinel-2 MGRS tile (100 km x 100 km).\n\n## Configuration & Resource Usage\n\nRun configurations for different ROI/TOI with memory requirements and estimated run durations.\n\n### Synchronous calls\n\nTODO: Replace with actual measurements!!!\n\n| Spatial extent | Run duration |\n|----------------|--------------|\n| 100 m x 100 m  | 1 minute     |\n| 500m x 500 m   | 1 minute     |\n| 1 km x 1 km    | 1 minute     |\n| 5 km x 5 km    | 2 minutes    |\n| 10 km x 10 km  | 3 minutes    |\n| 50 km x 50 km  | 9 minutes    |\n\nThe maximum duration of a synchronous run is 15 minutes.\nFor long running computations, you can use batch jobs.\n\n### Batch jobs\n\nTODO: Replace with actual measurements!!!\n\n| Spatial extent  | Temporal extent | Executor memory | Run duration |\n|-----------------|-----------------|-----------------|--------------|\n| 100 m x 100 m   | 1 month         | default         | 7 minutes    |\n| 500 m x 100 m   | 1 month         | default         | 7 minutes    |\n| 1 km x 1 km     | 1 month         | default         | 7 minutes    |\n| 5 km x 5 km     | 1 month         | default         | 10 minutes   |\n| 10 km x 10 km   | 1 month         | default         | 11 minutes   |\n| 50 km x 50 km   | 1 month         | 6 GB            | 20 minutes   |\n| 100 km x 100 km | 1 month         | 7 GB            | 34 minutes   |\n| 100m x 100 m    | 7 months        | default         | 10 minutes   |\n| 500 m x 500 m   | 7 months        | default         | 10 minutes   |\n| 1 km x 1 km     | 7 months        | default         | 14 minutes   |\n| 5 km x 5 km     | 7 months        | default         | 14 minutes   |\n| 10 km x 10 km   | 7 months        | default         | 19 minutes   |\n| 50 km x 50 km   | 7 months        | 6 GB            | 45 minutes   |\n| 100 km x 100 km | 7 months        | 8 GB            | 65 minutes   |\n",
    "parameters": [
        {
            "name": "input_raster_cube",
            "description": "Raster cube for which to calculate the peaks and valleys",
            "schema": {
                "type": "object",
                "subtype": "datacube"
            }
        }
    ]
}