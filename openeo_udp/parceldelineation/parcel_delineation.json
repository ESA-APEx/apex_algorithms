{
  "process_graph": {
    "loadcollection1": {
      "process_id": "load_collection",
      "arguments": {
        "bands": [
          "B04",
          "B08"
        ],
        "id": "SENTINEL2_L2A",
        "properties": {
          "eo:cloud_cover": {
            "process_graph": {
              "lte1": {
                "process_id": "lte",
                "arguments": {
                  "x": {
                    "from_parameter": "value"
                  },
                  "y": 10
                },
                "result": true
              }
            }
          }
        },
        "spatial_extent": {
          "from_parameter": "spatial_extent"
        },
        "temporal_extent": {
          "from_parameter": "temporal_extent"
        }
      }
    },
    "ndvi1": {
      "process_id": "ndvi",
      "arguments": {
        "data": {
          "from_node": "loadcollection1"
        },
        "nir": "B08",
        "red": "B04"
      }
    },
    "applyneighborhood1": {
      "process_id": "apply_neighborhood",
      "arguments": {
        "data": {
          "from_node": "ndvi1"
        },
        "overlap": [
          {
            "dimension": "x",
            "value": 32,
            "unit": "px"
          },
          {
            "dimension": "y",
            "value": 32,
            "unit": "px"
          }
        ],
        "process": {
          "process_graph": {
            "runudf1": {
              "process_id": "run_udf",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "runtime": "Python",
                "udf": "# /// script\n# dependencies = [\n# \"onnxruntime\",\n# ]\n# ///\n\nimport functools\nimport gc\nimport sys\nimport os\nfrom typing import Dict\nimport random\nimport xarray as xr\nfrom openeo.udf import inspect\nimport requests\nimport zipfile\nimport onnxruntime as ort\n\n\nmodel_names = frozenset([\n    \"BelgiumCropMap_unet_3BandsGenerator_Network1.onnx\",\n    \"BelgiumCropMap_unet_3BandsGenerator_Network2.onnx\",\n    \"BelgiumCropMap_unet_3BandsGenerator_Network3.onnx\",\n])\n\n# Fixed directories for dependencies and model files\nMODEL_DIR = \"model_files\"\n\n\ndef download_file(url, path):\n    \"\"\"\n    Downloads a file from the given URL to the specified path.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    with open(path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef extract_zip(zip_path, extract_to):\n    \"\"\"\n    Extracts a zip file from zip_path to the specified extract_to directory.\n    \"\"\"\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_to)\n    os.remove(zip_path)  # Clean up the zip file after extraction\n\n\ndef add_directory_to_sys_path(directory):\n    \"\"\"\n    Adds a directory to the Python sys.path if it's not already present.\n    \"\"\"\n    if directory not in sys.path:\n        sys.path.insert(0, directory)\n\n\n@functools.lru_cache(maxsize=5)\ndef setup_model_and_dependencies(model_url):\n    \"\"\"\n    Main function to set up the model and dependencies by downloading, extracting,\n    and adding necessary directories to sys.path.\n    \"\"\"\n    inspect(message=\"Create directories\")\n    # Ensure base directories exist\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    # Download and extract model if not already present\n    # if the drectory is empty\n    if not os.listdir(MODEL_DIR):\n        inspect(message=\"Extract model\")\n        zip_path = os.path.join(MODEL_DIR, \"temp.zip\")\n        download_file(model_url, zip_path)\n        # inspect(message=\"Model file downloaded\")\n        extract_zip(zip_path, MODEL_DIR)\n        # Add the extracted dependencies directory to sys.path\n        add_directory_to_sys_path(MODEL_DIR)\n    inspect(message=\"Done creating directories\")\n\nsetup_model_and_dependencies(\n    model_url=\"https://artifactory.vgt.vito.be:443/artifactory/auxdata-public/openeo/parcelDelination/BelgiumCropMap_unet_3BandsGenerator_Models.zip\")\n    #dependencies_url=\"https://artifactory.vgt.vito.be:443/auxdata-public/openeo/onnx_dependencies.zip\"\n# )\n\n# Add dependencies to the Python path\n# import onnxruntime as ort  # Import after downloading dependencies\n\n\n@functools.lru_cache(maxsize=25)\ndef load_ort_sessions(names):\n    \"\"\"\n    Load the models and make the prediction functions.\n    The lru_cache avoids loading the model multiple times on the same worker.\n\n    @param modeldir: Model directory\n    @return: Loaded model sessions\n    \"\"\"\n    inspect(message=\"Loading convolutional neural networks as ONNX runtime sessions ...\")\n    model_sessions = [\n        ort.InferenceSession(f\"{MODEL_DIR}/{model_name}\")\n        for model_name in names\n    ]\n    inspect(message=\"DONE Loading convolutional neural networks as ONNX runtime sessions.\")\n    return model_sessions\n\n\ndef process_window_onnx(ndvi_stack: xr.DataArray, patch_size=128):\n    \"\"\"Compute predictions.\n\n    Compute predictions using ML models. ML models takes three inputs images and predicts\n    one image. Four predictions are made per model using three random images. Three images\n    are considered to save computational time. Final result is median of these predictions.\n\n    Parameters\n    ----------\n    ndvi_stack : DataArray\n        ndvi data\n    patch_size : Int\n        Size of the sample\n\n    \"\"\"\n    # we'll do 12 predictions: use 3 networks, and for each random take 3 NDVI images and repeat 4 times\n    ort_sessions = load_ort_sessions(model_names)    # get models\n\n    inspect(message=\"Something is happening after loading ML\")\n    predictions_per_model = 4\n    no_rand_images = 3          # Number of random images that are needed for input\n    no_images = ndvi_stack.t.shape[0]\n    \n    # Range of index of images\n    _range = range(no_images)\n    # List of all predictions\n    prediction = []\n    for model_index, ort_session in enumerate(ort_sessions):\n        # make 4 predictions per model\n        for i in range(predictions_per_model):\n            # initialize a predicter array\n            random.seed(i)   # without seed we will have random number leading to non-reproducable results.\n            _idx = random.choices(_range, k=no_rand_images) # Random selection of 3 images for input\n            # re-shape the input data for ML input \n            input_data = ndvi_stack.isel(t=_idx).data.reshape(1, patch_size * patch_size, no_rand_images)\n            ort_inputs = {ort_session.get_inputs()[0].name: input_data}\n\n            # Run ML to predict\n            ort_outputs = ort_session.run(None, ort_inputs)\n            # reshape ort_outputs and append it to prediction list\n            prediction.append(ort_outputs[0].reshape((patch_size, patch_size)))\n\n    # free up some memory to avoid memory errors\n    gc.collect()\n\n    # Create a DataArray of all predictions\n    all_predictions = xr.DataArray(prediction, dims=[\"predict\", \"x\", \"y\"],\n                                   coords={\"predict\": range(len(prediction)),\n                                           \"x\": ndvi_stack.coords[\"x\"],\n                                           \"y\": ndvi_stack.coords[\"y\"]}\n                                   )\n    # final prediction is the median of all predictions per pixel\n    return all_predictions.median(dim=\"predict\")\n\n\ndef preprocess_datacube(cubearray: xr.DataArray, min_images: int):\n    # check if bands is in the dims and select the first index\n    if \"bands\" in cubearray.dims:\n        nvdi_stack = cubearray.isel(bands=0)\n    else:\n        nvdi_stack = cubearray\n\n    # Clamp out of range NDVI values\n    nvdi_stack = nvdi_stack.where(lambda nvdi_stack: nvdi_stack < 0.92, 0.92)\n    nvdi_stack = nvdi_stack.where(lambda nvdi_stack: nvdi_stack > -0.08)       # No data exists id less than -0.08\n    nvdi_stack += 0.08\n\n    # Fill the no data with value 0\n    nvdi_stack_data = nvdi_stack.fillna(0)\n\n    # Count the amount of invalid data per acquisition\n    sum_invalid = nvdi_stack.isnull().sum(dim=['x', 'y'])\n\n    # Select all clear images (without ANY missing values)\n    # or select the 3 best ones (contain nans)\n    if (sum_invalid.data == 0).sum() > min_images:\n        good_data = nvdi_stack_data.sel(t = sum_invalid[sum_invalid.data == 0].t)\n    else:\n        good_data = nvdi_stack_data.sel(t = sum_invalid.sortby(sum_invalid).t[:min_images])\n    return good_data.transpose(\"x\", \"y\", \"t\")\n\n\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\n    # select atleast best 3 temporal images of ndvi for ML\n    min_images = 3\n    inspect(message=\"START Preprocessing Done!\")\n    # preprocess the datacube\n    ndvi_stack = preprocess_datacube(cube, min_images)\n    inspect(message=\"Preprocessing Done!\")\n    # check number of images after preprocessing\n    # if the stack doesn't have at least 3 temporal images, we cannot process this window\n    nr_valid_images = ndvi_stack.t.shape[0]\n    if nr_valid_images < min_images:\n        inspect(message=\"Not enough input data for this window -> skipping!\")\n        return None\n    \n    inspect(message=\"START ML!\")\n    # process the window\n    result = process_window_onnx(ndvi_stack)\n\n    # Reintroduce time and bands dimensions\n    result_xarray = result.expand_dims(dim={\"t\": [(cube.t.dt.year.values[0])], \"bands\": [\"prediction\"]})\n\n    # Return the resulting xarray\n    return result_xarray\n"
              },
              "result": true
            }
          }
        },
        "size": [
          {
            "dimension": "x",
            "value": 64,
            "unit": "px"
          },
          {
            "dimension": "y",
            "value": 64,
            "unit": "px"
          }
        ]
      }
    },
    "applyneighborhood2": {
      "process_id": "apply_neighborhood",
      "arguments": {
        "data": {
          "from_node": "applyneighborhood1"
        },
        "overlap": [
          {
            "dimension": "x",
            "value": 0,
            "unit": "px"
          },
          {
            "dimension": "y",
            "value": 0,
            "unit": "px"
          }
        ],
        "process": {
          "process_graph": {
            "runudf2": {
              "process_id": "run_udf",
              "arguments": {
                "data": {
                  "from_parameter": "data"
                },
                "runtime": "Python",
                "udf": "from xarray import DataArray\nfrom skimage import segmentation, graph\nfrom skimage.filters import sobel\nfrom typing import Dict\n\n\ndef apply_datacube(cube: DataArray, context: Dict) -> DataArray:\n    # get the underlying array without the bands and t dimension\n    _data = cube.squeeze(\"t\", drop=True).squeeze(\"bands\", drop=True).values\n    # compute edges\n    edges = sobel(_data)\n    # Perform felzenszwalb segmentation\n    segment = segmentation.felzenszwalb(_data, scale=120, sigma=0.0, min_size=30, channel_axis=None)\n    # Perform the rag boundary analysis and merge the segments\n    bgraph = graph.rag_boundary(segment, edges)\n    # merging segments\n    mergedsegment = graph.cut_threshold(segment, bgraph, 0.15, in_place=False)\n    # create a data cube and perform masking operations\n    output_arr = DataArray(mergedsegment.reshape(cube.shape), dims=cube.dims, coords=cube.coords)\n    output_arr = output_arr.where(cube >= 0.3)   # Mask the output pixels based on the cube values <0.3\n    output_arr = output_arr.where(output_arr >= 0)  # Mask all values less than or equal to zero\n    return output_arr\n"
              },
              "result": true
            }
          }
        },
        "size": [
          {
            "dimension": "x",
            "value": 2048,
            "unit": "px"
          },
          {
            "dimension": "y",
            "value": 2048,
            "unit": "px"
          }
        ]
      },
      "result": true
    }
  },
  "id": "Parcel Delineation",
  "summary": " Parcel delineation using Sentinel-2 data retrived from the CDSE and processed on openEO.",
  "description": "# Parcel delineation\nThis is an [openEO](https://openeo.org/) example for delineating agricultural parcels based on a neural network, using Sentinel-2 input data.\n\nThe example focuses on the inference step, using a trained model. It demonstrates data loading and preprocessing, \ninference, and finally producing vector data as a result.\n\nThe example serves as a technology demonstration for openEO, and is not intended for use in another context. \nPlease contact the authors in case you're interested in a field detector!\n\n## Authors\n\n- Kristof Van Tricht\n- Jeroen Dries\n- Victor Verhaert\n\nTuning by:\n- Kasper Bonte\n- Bart Driessen\n\n[VITO Remote Sensing](https://remotesensing.vito.be)\n\n## Running the example\n\nMost openEO providers require an account to run the example. They offer various options for this, often providing you with some trial to test out the platform.\nYou can find all known openEO providers on the [openEO hub](https://hub.openeo.org/).\n\n",
  "parameters": [
    {
      "name": "spatial_extent",
      "description": "Spatial extent specified as a bounding box with 'west', 'south', 'east' and 'north' fields.",
      "schema": {
        "type": "object",
        "subtype": "bounding-box",
        "required": [
          "west",
          "south",
          "east",
          "north"
        ],
        "properties": {
          "west": {
            "type": "number",
            "description": "West (lower left corner, coordinate axis 1)."
          },
          "south": {
            "type": "number",
            "description": "South (lower left corner, coordinate axis 2)."
          },
          "east": {
            "type": "number",
            "description": "East (upper right corner, coordinate axis 1)."
          },
          "north": {
            "type": "number",
            "description": "North (upper right corner, coordinate axis 2)."
          },
          "crs": {
            "description": "Coordinate reference system of the extent, specified as as [EPSG code](http://www.epsg-registry.org/) or [WKT2 CRS string](http://docs.opengeospatial.org/is/18-010r7/18-010r7.html). Defaults to `4326` (EPSG code 4326) unless the client explicitly requests a different coordinate reference system.",
            "anyOf": [
              {
                "type": "integer",
                "subtype": "epsg-code",
                "title": "EPSG Code",
                "minimum": 1000
              },
              {
                "type": "string",
                "subtype": "wkt2-definition",
                "title": "WKT2 definition"
              }
            ],
            "default": 4326
          }
        }
      },
      "default": {
        "west": 5.0,
        "south": 51.2,
        "east": 5.1,
        "north": 51.3
      },
      "optional": true
    },
    {
      "name": "temporal_extent",
      "description": "Temporal extent specified as two-element array with start and end date/date-time.",
      "schema": {
        "type": "array",
        "subtype": "temporal-interval",
        "uniqueItems": true,
        "minItems": 2,
        "maxItems": 2,
        "items": {
          "anyOf": [
            {
              "type": "string",
              "subtype": "date-time",
              "format": "date-time"
            },
            {
              "type": "string",
              "subtype": "date",
              "format": "date"
            },
            {
              "type": "null"
            }
          ]
        }
      },
      "default": [
        "2021-01-01",
        "2021-12-31"
      ],
      "optional": true
    }
  ]
}